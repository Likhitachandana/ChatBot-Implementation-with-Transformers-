# ChatBot-Implementation-with-Transformers-
To construct the Seq2Seq AI Chatbot, we employed an encoder-decoder attention mechanism architecture. Indeed, it is a typical method for creating Seq2Seq AI Chatbots. Two RNNs—an encoder RNN and a decoder RNN—make up the encoder-decoder attention mechanism architecture. 
We have divided our entire project into 3 updates:
In the first update, we took 132 samples about yale Graduate programs and worked on DATA Preprocessing 
In the second update, we have used T5 transformers model to fine-tune our model on our custom dataset and in the final update, we have implemented the Hugging face model for fine-tuning according to the dataset and performed the Final Model Evaluation on it. 
